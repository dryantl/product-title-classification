{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Useful Modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "config = tensorflow.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5 #(misalnya kalo pengen 0.4 dari GPU memory)\n",
    "session = tensorflow.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program\\Anaconda\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from time import gmtime, strftime\n",
    "import time\n",
    "import datetime\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# # Preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# Embedder\n",
    "from gensim.models import FastText\n",
    "\n",
    "# Classifier\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.ensemble import GradientBoostingClassifier as GBC\n",
    "# from sklearn.linear_model import LogisticRegression as LR\n",
    "# from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "# from sklearn.grid_search import GridSearchCV as GS\n",
    "from xgboost import XGBClassifier as XGB\n",
    "from sklearn.decomposition import PCA,TruncatedSVD\n",
    "from sklearn.preprocessing import OneHotEncoder as OHE\n",
    "\n",
    "# import keras\n",
    "# import tensorflow\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras import regularizers\n",
    "\n",
    "from preprocessing_pipeline import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine Model's Save Location\n",
    "\n",
    "#version=\n",
    "version=\"version_x\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(not(os.path.exists(\"model/{}\".format(version)))):\n",
    "    os.makedirs(\"model/{}\".format(version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing_pipeline import preprocessing\n",
    "\n",
    "with open(\"model/{}/word_embedder.pickle\".format(version), \"rb\") as file:\n",
    "    word_embedder = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<preprocessing_pipeline.preprocessing at 0xcb22549208>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor=preprocessing(word_embedder.vector_size,word_embedder)\n",
    "preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data To Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_product_title_data():\n",
    "    product_title_only=pd.read_csv('data/query.csv',header=None)\n",
    "    product_title_only.dropna(inplace=True,axis=0)\n",
    "    features=product_title_only[1]\n",
    "    labels=product_title_only[0]\n",
    "    \n",
    "    \n",
    "#     return (product_title_only[:300000],\n",
    "#             product_title_only[300000:600000],\n",
    "#             product_title_only[600000:900000],\n",
    "#             product_title_only[900000:1200000],\n",
    "#             product_title_only[1200000:1500000],\n",
    "#             product_title_only[1500000:1800000],\n",
    "#             product_title_only[1800000:])\n",
    "    \n",
    "    return features,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_product_title_data_small():\n",
    "    product_title_only=pd.read_csv('data/big.csv',header=None)\n",
    "    product_title_only.dropna(inplace=True,axis=0)\n",
    "    features=product_title_only[1]\n",
    "    labels=product_title_only[0]\n",
    "    \n",
    "#     return (product_title_only[:300000],\n",
    "#             product_title_only[300000:600000],\n",
    "#             product_title_only[600000:900000],\n",
    "#             product_title_only[900000:1200000],\n",
    "#             product_title_only[1200000:1500000],\n",
    "#             product_title_only[1500000:1800000],\n",
    "#             product_title_only[1800000:])\n",
    "    \n",
    "    return features,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(sentences):\n",
    "    counts = dict()\n",
    "    print(\"1/1\")\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            if word in counts:\n",
    "                counts[word] += 1\n",
    "            else:\n",
    "                counts[word] = 1\n",
    "    return counts\n",
    "\n",
    "def class_count(words):\n",
    "    counts = dict()\n",
    "    print(\"1/1\")\n",
    "    for word in words:\n",
    "        if word in counts:\n",
    "            counts[word] += 1\n",
    "        else:\n",
    "            counts[word] = 1\n",
    "    return counts\n",
    "\n",
    "def getFilteredData(product_title,labels,frequency, N_words, word_length):\n",
    "    print(\"1/3\")\n",
    "    result=word_count(product_title)\n",
    "    print(\"2/3\")\n",
    "    new_product_title=[]\n",
    "    for sentence in tqdm.tqdm(product_title):\n",
    "        new_product_title.append([word for word in sentence if result[word]>=frequency and len(word)>=word_length])\n",
    "    \n",
    "    print(\"3/3\")\n",
    "    new_features=[]\n",
    "    new_labels=[]\n",
    "    for index,title in tqdm.tqdm(enumerate(new_product_title)):\n",
    "        if(len(title)>=N_words):\n",
    "            new_features.append(title)\n",
    "            new_labels.append(labels[index])\n",
    "    \n",
    "    return new_features,new_labels\n",
    "\n",
    "def getTfIdf(new_product_title):\n",
    "    print(\"1/3\")\n",
    "    concatenated_product_title=[]\n",
    "    for sentence in tqdm.tqdm(new_product_title):\n",
    "        concatenated_product_title.append(\" \".join(sentence))\n",
    "    print(\"2/3\")\n",
    "    cv=CountVectorizer()\n",
    "    result=cv.fit_transform(concatenated_product_title)\n",
    "    print(\"3/3\")\n",
    "    tftransformer = TfidfTransformer(smooth_idf=False)\n",
    "    final_result=tftransformer.fit_transform(result)\n",
    "    \n",
    "    return final_result,cv,tftransformer\n",
    "\n",
    "def getTfIdfCustom(new_product_title,vocab):\n",
    "    print(\"1/3\")\n",
    "    concatenated_product_title=[]\n",
    "    for sentence in tqdm.tqdm(new_product_title):\n",
    "        concatenated_product_title.append(\" \".join(sentence))\n",
    "    print(\"2/3\")\n",
    "    cv=CountVectorizer(vocabulary=vocab)\n",
    "    result=cv.fit_transform(concatenated_product_title)\n",
    "    print(\"3/3\")\n",
    "    tftransformer = TfidfTransformer(smooth_idf=False)\n",
    "    final_result=tftransformer.fit_transform(result)\n",
    "    \n",
    "    return final_result,cv,tftransformer\n",
    "\n",
    "def getFilteredClasses(product_title,labels,top_N):\n",
    "    print(\"1/3\")\n",
    "    sorted_by_value = sorted(class_count(labels).items(), key=lambda kv: kv[1])\n",
    "    valid_class=[value[0] for value in sorted_by_value[-top_N:]]\n",
    "    print(\"2/3\")\n",
    "    product_title=list(product_title)\n",
    "    new_features=[]\n",
    "    new_labels=[]\n",
    "    for index,label in tqdm.tqdm(enumerate(labels)):\n",
    "        if(label in valid_class):\n",
    "            new_labels.append(label)\n",
    "            new_features.append(product_title[index])\n",
    "    \n",
    "    return new_features,new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count_label(sentences,labels,target):\n",
    "    counts = dict()\n",
    "    print(\"1/1\")\n",
    "    for index,sentence in enumerate(sentences):\n",
    "        if(labels[index]==target):\n",
    "            for word in sentence:\n",
    "                if word in counts:\n",
    "                    counts[word] += 1\n",
    "                else:\n",
    "                    counts[word] = 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_embedding=read_product_title_data_small()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1\n",
      "0.6002772995416855\n"
     ]
    }
   ],
   "source": [
    "sorted_by_value = sorted(class_count(data_for_embedding[1]).items(), key=lambda kv: kv[1])\n",
    "valid_class=np.sum([value[1] for value in sorted_by_value[-30:]])\n",
    "print(valid_class/len(data_for_embedding[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3\n",
      "1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2440682it [00:15, 157900.71it/s]\n"
     ]
    }
   ],
   "source": [
    "nfnf,nlnl=getFilteredClasses(data_for_embedding[0],data_for_embedding[1],300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  ---------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1465086/1465086 [00:17<00:00, 84487.07it/s]\n"
     ]
    }
   ],
   "source": [
    "product_title=[preprocessor.remove_parentheses(value) for value in tqdm.tqdm(nfnf)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3\n",
      "1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 15967/1465086 [00:00<00:41, 34596.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1465086/1465086 [00:04<00:00, 328552.05it/s]\n",
      "203679it [00:00, 1018160.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1465086it [00:00, 1620229.29it/s]\n"
     ]
    }
   ],
   "source": [
    "new_data=getFilteredData(product_title,list(nlnl),50,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/30 [00:00<00:05,  5.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1\n",
      "1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3/30 [00:00<00:04,  6.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1\n",
      "1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 5/30 [00:00<00:04,  5.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1\n",
      "1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 8/30 [00:01<00:03,  6.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1\n",
      "1/1\n",
      "1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 10/30 [00:01<00:02,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1\n",
      "1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 12/30 [00:01<00:02,  6.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1\n",
      "1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 14/30 [00:02<00:02,  6.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1\n",
      "1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 15/30 [00:02<00:02,  6.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 18/30 [00:02<00:01,  6.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1\n",
      "1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 21/30 [00:03<00:01,  6.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1\n",
      "1/1\n",
      "1/1\n",
      "1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 24/30 [00:03<00:00,  6.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1\n",
      "1/1\n",
      "1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 26/30 [00:03<00:00,  7.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1\n",
      "1/1\n",
      "1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 29/30 [00:04<00:00,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1\n",
      "1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:04<00:00,  7.03it/s]\n"
     ]
    }
   ],
   "source": [
    "fff=new_data[0]\n",
    "lll=new_data[1]\n",
    "\n",
    "top_3={}\n",
    "for label in tqdm.tqdm(set(lll)):\n",
    "    top_3[label]=[value[0] for value in sorted(word_count_label(fff,lll,label).items(), key=lambda x: x[1])[-30:]]\n",
    "    \n",
    "allw=[]\n",
    "for label in list(top_3):\n",
    "    for i in top_3[label]:\n",
    "        allw.append(i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 343091/1030967 [00:00<00:00, 1715104.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1030967/1030967 [00:00<00:00, 1515835.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/3\n",
      "3/3\n"
     ]
    }
   ],
   "source": [
    "aaaaa=getTfIdfCustom(new_data[0],set(allw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1030967it [03:23, 5074.99it/s]\n"
     ]
    }
   ],
   "source": [
    "newnew=[]\n",
    "for index,value in tqdm.tqdm(enumerate(aaaaa[0].toarray())):\n",
    "    temp=list(value)+list(preprocessor.vectorize_sentence(new_data[0][index]))\n",
    "    newnew.append(temp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "llee=LabelEncoder()\n",
    "wenwen=llee.fit_transform(lll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_X_train,nn_X_test,nn_y_train,nn_y_test=train_test_split(newnew,to_categorical(wenwen),test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1000, input_shape=(len(nn_X_train[0]),), activation='relu'))\n",
    "model.add(Dense(750, activation='relu'))\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dense(len(nn_y_train[0]), activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(optimizer=\"Adagrad\", loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 927870 samples, validate on 103097 samples\n",
      "Epoch 1/25\n",
      "927870/927870 [==============================] - 34s 37us/step - loss: 0.5808 - acc: 0.8321 - val_loss: 0.5025 - val_acc: 0.8504\n",
      "Epoch 2/25\n",
      "927870/927870 [==============================] - 33s 36us/step - loss: 0.4546 - acc: 0.8620 - val_loss: 0.4589 - val_acc: 0.8611\n",
      "Epoch 3/25\n",
      "927870/927870 [==============================] - 34s 37us/step - loss: 0.4090 - acc: 0.8725 - val_loss: 0.4363 - val_acc: 0.8668\n",
      "Epoch 4/25\n",
      "927870/927870 [==============================] - 34s 37us/step - loss: 0.3795 - acc: 0.8789 - val_loss: 0.4222 - val_acc: 0.8706\n",
      "Epoch 5/25\n",
      "927870/927870 [==============================] - 40s 43us/step - loss: 0.3588 - acc: 0.8835 - val_loss: 0.4101 - val_acc: 0.8724\n",
      "Epoch 6/25\n",
      "927870/927870 [==============================] - 43s 47us/step - loss: 0.3426 - acc: 0.8871 - val_loss: 0.4022 - val_acc: 0.8740\n",
      "Epoch 7/25\n",
      "927870/927870 [==============================] - 43s 46us/step - loss: 0.3299 - acc: 0.8901 - val_loss: 0.3987 - val_acc: 0.8752\n",
      "Epoch 8/25\n",
      "927870/927870 [==============================] - 44s 47us/step - loss: 0.3198 - acc: 0.8920 - val_loss: 0.3927 - val_acc: 0.8774\n",
      "Epoch 9/25\n",
      "927870/927870 [==============================] - 45s 49us/step - loss: 0.3112 - acc: 0.8939 - val_loss: 0.3881 - val_acc: 0.8791\n",
      "Epoch 10/25\n",
      "927870/927870 [==============================] - 50s 54us/step - loss: 0.3038 - acc: 0.8954 - val_loss: 0.3856 - val_acc: 0.8792\n",
      "Epoch 11/25\n",
      "927870/927870 [==============================] - 43s 46us/step - loss: 0.2977 - acc: 0.8968 - val_loss: 0.3821 - val_acc: 0.8806\n",
      "Epoch 12/25\n",
      "927870/927870 [==============================] - 35s 37us/step - loss: 0.2924 - acc: 0.8979 - val_loss: 0.3806 - val_acc: 0.8795\n",
      "Epoch 13/25\n",
      "927870/927870 [==============================] - 42s 45us/step - loss: 0.2876 - acc: 0.8988 - val_loss: 0.3783 - val_acc: 0.8816\n",
      "Epoch 14/25\n",
      "927870/927870 [==============================] - 43s 46us/step - loss: 0.2837 - acc: 0.8998 - val_loss: 0.3767 - val_acc: 0.8812\n",
      "Epoch 15/25\n",
      "927870/927870 [==============================] - 43s 46us/step - loss: 0.2801 - acc: 0.9002 - val_loss: 0.3748 - val_acc: 0.8825\n",
      "Epoch 16/25\n",
      "927870/927870 [==============================] - 44s 47us/step - loss: 0.2769 - acc: 0.9010 - val_loss: 0.3719 - val_acc: 0.8827\n",
      "Epoch 17/25\n",
      "927870/927870 [==============================] - 51s 55us/step - loss: 0.2739 - acc: 0.9014 - val_loss: 0.3729 - val_acc: 0.8815\n",
      "Epoch 18/25\n",
      "927870/927870 [==============================] - 46s 49us/step - loss: 0.2713 - acc: 0.9020 - val_loss: 0.3705 - val_acc: 0.8828\n",
      "Epoch 19/25\n",
      "927870/927870 [==============================] - 43s 47us/step - loss: 0.2690 - acc: 0.9024 - val_loss: 0.3693 - val_acc: 0.8836\n",
      "Epoch 20/25\n",
      "927870/927870 [==============================] - 44s 47us/step - loss: 0.2668 - acc: 0.9026 - val_loss: 0.3706 - val_acc: 0.8834\n",
      "Epoch 21/25\n",
      "927870/927870 [==============================] - 43s 46us/step - loss: 0.2650 - acc: 0.9031 - val_loss: 0.3696 - val_acc: 0.8831\n",
      "Epoch 22/25\n",
      "927870/927870 [==============================] - 43s 46us/step - loss: 0.2631 - acc: 0.9034 - val_loss: 0.3687 - val_acc: 0.8838\n",
      "Epoch 23/25\n",
      "927870/927870 [==============================] - 49s 52us/step - loss: 0.2615 - acc: 0.9037 - val_loss: 0.3682 - val_acc: 0.8833\n",
      "Epoch 24/25\n",
      "212400/927870 [=====>........................] - ETA: 31s - loss: 0.2535 - acc: 0.9063"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-b72c1c168c3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnn_X_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn_y_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnn_X_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnn_y_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/src/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/src/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/src/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/src/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                                 session)\n\u001b[0;32m-> 2631\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit([nn_X_train], nn_y_train, initial_epoch=0, epochs=25, batch_size=100, validation_data=([nn_X_test],nn_y_test), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-d57b7c3453cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_X_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/src/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1150\u001b[0m                              'argument.')\n\u001b[1;32m   1151\u001b[0m         \u001b[0;31m# Validate user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/src/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    752\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/src/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0;34m'Expected to see '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' array(s), '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0;34m'but instead got the following list of '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                 str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             raise ValueError(\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/numpy/core/arrayprint.py\u001b[0m in \u001b[0;36marray_repr\u001b[0;34m(arr, max_line_width, precision, suppress_small)\u001b[0m\n\u001b[1;32m   1392\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1393\u001b[0m         lst = array2string(arr, max_line_width, precision, suppress_small,\n\u001b[0;32m-> 1394\u001b[0;31m                            ', ', prefix, suffix=suffix)\n\u001b[0m\u001b[1;32m   1395\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# show zero-length shape unless it is (0,)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1396\u001b[0m         \u001b[0mlst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"[], shape=%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/numpy/core/arrayprint.py\u001b[0m in \u001b[0;36marray2string\u001b[0;34m(a, max_line_width, precision, suppress_small, separator, prefix, style, formatter, threshold, edgeitems, sign, floatmode, suffix, **kwarg)\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"[]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_array2string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseparator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/numpy/core/arrayprint.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0mrepr_running\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0mrepr_running\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/numpy/core/arrayprint.py\u001b[0m in \u001b[0;36m_array2string\u001b[0;34m(a, options, separator, prefix)\u001b[0m\n\u001b[1;32m    459\u001b[0m     lst = _formatArray(a, format_function, options['linewidth'],\n\u001b[1;32m    460\u001b[0m                        \u001b[0mnext_line_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseparator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'edgeitems'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m                        summary_insert, options['legacy'])\n\u001b[0m\u001b[1;32m    462\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/numpy/core/arrayprint.py\u001b[0m in \u001b[0;36m_formatArray\u001b[0;34m(a, format_function, line_width, next_line_prefix, separator, edge_items, summary_insert, legacy)\u001b[0m\n\u001b[1;32m    759\u001b[0m         return recurser(index=(),\n\u001b[1;32m    760\u001b[0m                         \u001b[0mhanging_indent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnext_line_prefix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m                         curr_width=line_width)\n\u001b[0m\u001b[1;32m    762\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0;31m# recursive closures have a cyclic reference to themselves, which\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/numpy/core/arrayprint.py\u001b[0m in \u001b[0;36mrecurser\u001b[0;34m(index, hanging_indent, curr_width)\u001b[0m\n\u001b[1;32m    745\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrailing_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m                 nested = recurser(index + (-i,), next_hanging_indent,\n\u001b[0;32m--> 747\u001b[0;31m                                   next_width)\n\u001b[0m\u001b[1;32m    748\u001b[0m                 \u001b[0ms\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mhanging_indent\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnested\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mline_sep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/numpy/core/arrayprint.py\u001b[0m in \u001b[0;36mrecurser\u001b[0;34m(index, hanging_indent, curr_width)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhanging_indent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleading_items\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    701\u001b[0m                 \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecurser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_hanging_indent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m                 s, line = _extendLine(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pred=model.predict(nn_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.52      0.65      0.58       302\n",
      "          1       0.04      0.83      0.08         6\n",
      "          2       0.73      0.54      0.62      7291\n",
      "          3       0.78      0.68      0.72      8984\n",
      "          4       0.34      0.35      0.35       420\n",
      "          5       0.72      0.57      0.63      7886\n",
      "          6       0.67      0.61      0.64      8456\n",
      "          7       0.45      0.47      0.46       535\n",
      "          8       0.21      0.50      0.30       383\n",
      "          9       0.46      0.54      0.50       397\n",
      "         10       0.51      0.55      0.53       800\n",
      "         11       0.43      0.48      0.45       658\n",
      "         12       0.58      0.60      0.59       742\n",
      "         13       0.35      0.45      0.40       444\n",
      "         14       0.70      0.65      0.68      1182\n",
      "         15       0.08      1.00      0.15         4\n",
      "         16       0.81      0.86      0.83       527\n",
      "         17       0.53      0.47      0.50       283\n",
      "         18       0.09      0.41      0.15        32\n",
      "         19       0.10      0.17      0.13        94\n",
      "         20       0.62      0.82      0.70        45\n",
      "         21       0.76      0.83      0.80      1629\n",
      "         22       0.81      0.62      0.70      3332\n",
      "         23       0.63      0.77      0.69       277\n",
      "         24       0.69      0.80      0.74       114\n",
      "         25       0.24      0.62      0.35        69\n",
      "         26       0.86      0.84      0.85      1791\n",
      "         27       0.69      0.76      0.72      1356\n",
      "         28       0.05      1.00      0.09         1\n",
      "         29       0.16      0.56      0.25        18\n",
      "         30       0.28      0.30      0.29        27\n",
      "         31       0.58      0.79      0.67       115\n",
      "         32       0.82      0.78      0.80      1884\n",
      "         33       0.25      0.30      0.27        71\n",
      "         34       0.58      0.70      0.63       658\n",
      "         35       0.76      0.76      0.76       460\n",
      "         36       0.53      0.57      0.55        51\n",
      "         37       0.49      0.69      0.57       134\n",
      "         38       0.81      0.75      0.78      1389\n",
      "         39       0.30      0.52      0.38       181\n",
      "         40       0.43      0.79      0.56        24\n",
      "         41       0.52      0.73      0.61        49\n",
      "         42       0.18      0.28      0.22       105\n",
      "         43       0.03      0.11      0.04         9\n",
      "         44       0.59      0.61      0.60      1993\n",
      "         45       0.58      0.81      0.67       510\n",
      "         46       0.84      0.67      0.75        64\n",
      "         47       0.48      0.67      0.56       148\n",
      "         48       0.66      0.77      0.71       451\n",
      "         49       0.31      0.41      0.35       490\n",
      "         50       0.78      0.79      0.78       519\n",
      "         51       0.33      0.52      0.40       147\n",
      "         52       0.47      0.58      0.52       838\n",
      "         53       0.52      0.61      0.56      2401\n",
      "         54       0.35      0.25      0.29      5310\n",
      "         55       0.39      0.53      0.45       531\n",
      "         56       0.38      0.55      0.45       449\n",
      "         57       0.33      0.79      0.46        34\n",
      "         58       0.47      0.67      0.55        69\n",
      "         59       0.67      0.60      0.64        48\n",
      "         60       0.40      0.72      0.51        25\n",
      "         61       0.64      0.85      0.73       157\n",
      "         62       0.66      0.64      0.65       621\n",
      "         63       0.71      0.70      0.71      1398\n",
      "         64       0.00      0.00      0.00         0\n",
      "         65       0.66      0.68      0.67      3238\n",
      "         66       0.17      0.39      0.24        44\n",
      "         67       0.77      0.81      0.79       172\n",
      "         68       0.45      0.55      0.49        62\n",
      "         69       0.25      0.41      0.31       170\n",
      "         70       0.37      0.93      0.53        15\n",
      "         71       0.71      0.77      0.74      2121\n",
      "         72       0.32      0.33      0.33       189\n",
      "         73       0.65      0.79      0.71       532\n",
      "         74       0.14      0.25      0.18       728\n",
      "         75       0.35      0.71      0.47        58\n",
      "         76       0.65      0.74      0.69        42\n",
      "         77       0.87      0.82      0.85      9635\n",
      "         78       0.38      0.60      0.46       240\n",
      "         79       0.23      0.75      0.35         4\n",
      "         80       0.84      0.78      0.81       143\n",
      "         81       0.14      0.23      0.18       644\n",
      "         82       0.33      0.61      0.43       197\n",
      "         83       0.87      0.67      0.76      2679\n",
      "         84       0.48      0.63      0.55       225\n",
      "         85       0.52      0.68      0.59       100\n",
      "         86       0.52      0.71      0.60       560\n",
      "         87       0.91      0.76      0.83      3653\n",
      "         88       0.69      0.82      0.75       155\n",
      "         89       0.75      0.65      0.70       425\n",
      "         90       0.52      0.63      0.57        68\n",
      "         91       0.00      0.00      0.00       613\n",
      "         92       0.00      0.00      0.00         0\n",
      "         93       0.00      0.00      0.00       121\n",
      "         94       0.47      0.10      0.16       705\n",
      "         95       0.00      0.00      0.00       599\n",
      "         96       0.00      0.00      0.00       475\n",
      "         97       0.00      0.00      0.00       687\n",
      "         98       0.00      0.00      0.00        87\n",
      "         99       0.22      0.05      0.08       787\n",
      "        100       0.00      0.00      0.00       109\n",
      "        101       0.36      0.04      0.07      2453\n",
      "        102       0.00      0.00      0.00       697\n",
      "        103       0.00      0.00      0.00        43\n",
      "        104       0.21      0.23      0.22        44\n",
      "        105       0.00      0.00      0.00       357\n",
      "        106       0.01      0.00      0.00      1393\n",
      "        107       0.00      0.01      0.01       601\n",
      "        108       0.00      0.00      0.00         6\n",
      "        109       0.00      0.00      0.00        15\n",
      "        110       0.00      0.00      0.00         8\n",
      "        111       0.00      0.00      0.00         0\n",
      "        112       0.00      0.00      0.00      3619\n",
      "        113       0.00      0.01      0.01      1513\n",
      "        114       0.00      0.00      0.00      2085\n",
      "        115       0.00      0.00      0.00       179\n",
      "        116       0.00      0.00      0.00         8\n",
      "        117       0.00      0.00      0.00       931\n",
      "        118       0.00      0.00      0.00      7198\n",
      "        119       0.00      0.37      0.00        35\n",
      "        120       0.00      0.00      0.00       242\n",
      "        121       0.00      0.00      0.00         8\n",
      "        122       0.00      0.00      0.00         0\n",
      "        123       0.00      0.00      0.00       703\n",
      "        124       0.00      0.00      0.00       409\n",
      "        125       0.00      0.00      0.00       180\n",
      "        126       0.06      0.30      0.10        44\n",
      "        127       0.00      0.00      0.00      1621\n",
      "        128       0.00      0.00      0.00       427\n",
      "        129       0.00      0.00      0.00       133\n",
      "        130       0.00      0.00      0.00       438\n",
      "        131       0.00      0.00      0.00       240\n",
      "        132       0.00      0.00      0.00       114\n",
      "        133       0.00      0.00      0.00         1\n",
      "        134       0.00      0.00      0.00      2574\n",
      "        135       0.00      0.00      0.00         1\n",
      "        136       0.00      0.00      0.00      2407\n",
      "        137       0.00      0.00      0.00       661\n",
      "        138       0.00      0.00      0.00      1799\n",
      "        139       0.02      0.03      0.02       493\n",
      "        140       0.01      0.01      0.01      2372\n",
      "        141       0.00      0.00      0.00       254\n",
      "        142       0.00      0.00      0.00       166\n",
      "        143       0.01      0.01      0.01       349\n",
      "        144       0.00      0.00      0.00      2298\n",
      "        145       0.00      0.00      0.00       224\n",
      "        146       0.00      0.01      0.00        87\n",
      "        147       0.00      0.00      0.00        28\n",
      "        148       0.00      0.00      0.00        14\n",
      "        149       0.00      0.00      0.00        29\n",
      "        150       0.00      0.00      0.00        14\n",
      "        151       0.00      0.00      0.00       512\n",
      "        152       0.04      0.00      0.00       494\n",
      "        153       0.00      0.00      0.00       720\n",
      "        154       0.00      0.00      0.00       464\n",
      "        155       0.00      0.00      0.00      3019\n",
      "        156       0.00      0.00      0.00       157\n",
      "        157       0.00      0.02      0.00       137\n",
      "        158       0.03      0.14      0.05        42\n",
      "        159       0.00      0.00      0.00        18\n",
      "        160       0.00      0.00      0.00        77\n",
      "        161       0.04      0.01      0.02       146\n",
      "        162       0.00      0.00      0.00      1547\n",
      "        163       0.01      0.01      0.01       184\n",
      "        164       0.00      0.00      0.00         3\n",
      "        165       0.00      0.00      0.00       211\n",
      "        166       0.00      0.00      0.00         1\n",
      "        167       0.00      0.00      0.00        96\n",
      "        168       0.00      0.00      0.00       403\n",
      "        169       0.00      0.00      0.00       936\n",
      "        170       0.00      0.00      0.00         0\n",
      "        171       0.00      0.00      0.00       227\n",
      "        172       0.00      0.00      0.00         0\n",
      "        173       0.00      0.00      0.00       136\n",
      "        174       0.01      0.01      0.01       413\n",
      "        175       0.00      0.00      0.00        68\n",
      "        176       0.00      0.00      0.00        84\n",
      "        177       0.00      0.00      0.00        28\n",
      "        178       0.00      0.00      0.00        13\n",
      "        179       0.00      0.00      0.00       207\n",
      "        180       0.00      0.00      0.00        79\n",
      "        181       0.00      0.00      0.00        14\n",
      "        182       0.00      0.00      0.00       215\n",
      "        183       0.00      0.00      0.00        35\n",
      "        184       0.00      0.00      0.00       146\n",
      "        185       0.00      0.00      0.00       416\n",
      "        186       0.00      0.00      0.00        67\n",
      "        187       0.00      0.03      0.01        32\n",
      "        188       0.00      0.00      0.00         1\n",
      "        189       0.00      0.00      0.00        31\n",
      "        190       0.00      0.00      0.00         5\n",
      "        191       0.00      0.00      0.00      1980\n",
      "        192       0.00      0.00      0.00      1135\n",
      "        193       0.10      0.00      0.00      5311\n",
      "        194       0.00      0.00      0.00        41\n",
      "        195       0.00      0.00      0.00         1\n",
      "        196       0.00      0.00      0.00       421\n",
      "        197       0.00      0.00      0.00         1\n",
      "        198       0.00      0.00      0.00       217\n",
      "        199       0.00      0.00      0.00      2623\n",
      "        200       0.00      0.00      0.00        10\n",
      "        201       0.01      0.00      0.00     13341\n",
      "        202       0.00      0.01      0.00      1026\n",
      "        203       0.00      0.00      0.00       131\n",
      "        204       0.00      0.01      0.00       251\n",
      "        205       0.00      0.00      0.00        19\n",
      "        206       0.00      0.00      0.00       118\n",
      "        207       0.00      0.00      0.00        29\n",
      "        208       0.00      0.00      0.00        63\n",
      "        209       0.00      0.00      0.00       421\n",
      "        210       0.00      0.00      0.00       253\n",
      "        211       0.00      0.00      0.00       180\n",
      "        212       0.01      0.00      0.00      2567\n",
      "        213       0.00      0.00      0.00      1389\n",
      "        214       0.00      0.00      0.00       325\n",
      "        215       0.00      0.01      0.01      1236\n",
      "        216       0.00      0.00      0.00        83\n",
      "        217       0.00      0.00      0.00        95\n",
      "        218       0.00      0.00      0.00        61\n",
      "        219       0.00      0.00      0.00       991\n",
      "        220       0.00      0.00      0.00       650\n",
      "        221       0.00      0.00      0.00         2\n",
      "        222       0.00      0.00      0.00        12\n",
      "        223       0.00      0.00      0.00       856\n",
      "        224       0.00      0.00      0.00         1\n",
      "        225       0.00      0.00      0.00        14\n",
      "        226       0.00      0.00      0.00       455\n",
      "        227       0.00      0.00      0.00       340\n",
      "        228       0.00      0.00      0.00         0\n",
      "        229       0.00      0.00      0.00         0\n",
      "        230       0.00      0.00      0.00         0\n",
      "        231       0.00      0.00      0.00         0\n",
      "\n",
      "avg / total       0.36      0.33      0.34    187248\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(model.predict_classes([nn_X_test]),[np.argmax(value) for value in nn_y_test]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 147/2440682 [00:00<28:29, 1427.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REMOVING UNIMPORTANT CHARACTERS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2440682/2440682 [00:34<00:00, 71163.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APPLYING FILTER\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2440682/2440682 [00:07<00:00, 310338.14it/s]\n",
      "2440682it [00:01, 1439238.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODING LABELS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 490/1757951 [00:00<05:59, 4894.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONVERTING SENTENCE TO VECTOR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1757951/1757951 [04:00<00:00, 7302.99it/s]\n",
      "  0%|          | 0/300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE VECTOR TO PANDAS DATAFRAME\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [05:02<00:00,  1.01s/it]\n"
     ]
    }
   ],
   "source": [
    "#preprocess product title to 100-dimensional vector\n",
    "#and preprocess category name to integer label\n",
    "large_embedded_data, large_label_encoder = preprocessor.preprocess_data(\n",
    "    nfnf,\n",
    "    nlnl,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_embedded_data.head()\n",
    "large_embedded_data[\"sum\"]=large_embedded_data.drop([\"Labels\"],axis=1).sum(axis=1)\n",
    "large_embedded_data=large_embedded_data.loc[large_embedded_data[\"sum\"]!=0].drop(\"sum\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_X_train,nn_X_test,nn_y_train,nn_y_test=train_test_split(large_embedded_data.drop(\"Labels\",axis=1),to_categorical(large_embedded_data[\"Labels\"]),test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1000, input_shape=(300,), activation='relu'))\n",
    "model.add(Dense(750, activation='relu'))\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dense(232, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(optimizer=\"Adagrad\", loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1582081 samples, validate on 175787 samples\n",
      "Epoch 1/25\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(nn_X_train, nn_y_train, initial_epoch=0, epochs=25, batch_size=100, validation_data=(nn_X_test,nn_y_test), shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
